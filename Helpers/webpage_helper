import re
import requests
from urllib.parse import urlsplit
from collections import deque
from bs4 import BeautifulSoup
from Helpers import storage_helper

for item in storage_helper.get_all_urls():
    print(item)
    original_url = item[0]

    unscraped = deque([original_url])

    scraped = set()

    emails = set()
    linkedins = set()

    while len(unscraped):
        url = unscraped.popleft()
        scraped.add(url)
        parts = urlsplit(url)

        base_url = "{0.scheme}://{0.netloc}".format(parts)
        if '/' in parts.path:
            path = url[:url.rfind('/')+1]
        else:
            path = url

        print("Crawling URL %s" % url)
        try:
            response = requests.get(url)

        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):
            continue

        new_emails = set(re.findall(r"[a-z0-9\.\-+_]+@[a-z0-9\.\-+_]+\.com", response.text, re.I))
        new_linkedins = set(re.findall(r"(?:https?:)?\/\/(?:[\w]+\.)?linkedin\.com\/in\/(?P<permalink>[\w\-\_À-ÿ%]+)\/?", response.text, re.I))

        emails.update(new_emails)
        linkedins.update(new_linkedins)

        print(emails)
        print(linkedins)

        soup = BeautifulSoup(response.text, 'lxml')



